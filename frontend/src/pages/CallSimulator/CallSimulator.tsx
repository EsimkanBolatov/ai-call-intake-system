import React, { useState, useEffect, useRef } from "react";
import {
  Box, Typography, Paper, Button, Stack, Card, CardContent, Chip, LinearProgress
} from "@mui/material";
import { Mic, PhoneDisabled, RecordVoiceOver, SettingsVoice } from "@mui/icons-material";
import { io, Socket } from "socket.io-client";

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
const arrayBufferToBase64 = (buffer: ArrayBuffer): string => {
  let binary = '';
  const bytes = new Uint8Array(buffer);
  for (let i = 0; i < bytes.byteLength; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
};

const CallSimulator: React.FC = () => {
  // UI States
  const [status, setStatus] = useState<string>("–ì–æ—Ç–æ–≤ –∫ –∑–≤–æ–Ω–∫—É");
  const [isCalling, setIsCalling] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState<{role: string, text: string}[]>([]);
  const [incidentData, setIncidentData] = useState<any>(null);
  const [volume, setVolume] = useState(0);
  const [thresholdDisplay, setThresholdDisplay] = useState(0);
  const [calibrating, setCalibrating] = useState(false);

  // Refs
  const socketRef = useRef<Socket | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<AudioWorkletNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const audioChunksRef = useRef<Float32Array[]>([]);
  const aiAudioRef = useRef<HTMLAudioElement | null>(null);
  
  // VAD Refs
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const maxDurationTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null); // NEW: –¢–∞–π–º–µ—Ä –º–∞–∫—Å. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  const isSpeakingRef = useRef(false);
  const noiseLevelRef = useRef(10); // Default safety threshold
  const calibrationSamplesRef = useRef<number[]>([]);

  useEffect(() => {
    socketRef.current = io("http://localhost:3000", { 
        transports: ["websocket"],
        path: "/socket.io/"
    });

    socketRef.current.on("connect", () => console.log("Socket connected"));
    
    socketRef.current.on("ai-call-started", (data: { sessionId: string }) => {
        console.log("Call Started, Session ID:", data.sessionId);
        sessionIdRef.current = data.sessionId; 
        setStatus("üü¢ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –ì–æ–≤–æ—Ä–∏—Ç–µ...");
        setIsCalling(true);
        startListening();
    });

    socketRef.current.on("ai-response", (data) => {
        setStatus("ü§ñ AI –æ—Ç–≤–µ—á–∞–µ—Ç...");
        
        setTranscript(prev => [
            ...prev, 
            { role: 'user', text: data.text },
            { role: 'ai', text: data.response }
        ]);

        if (data.incident) setIncidentData(data.incident);

        if (data.audio) {
            const audioSrc = `data:audio/mp3;base64,${data.audio}`;
            if (aiAudioRef.current) {
                aiAudioRef.current.src = audioSrc;
                aiAudioRef.current.play();
                aiAudioRef.current.onended = () => {
                   setStatus("üéôÔ∏è –°–ª—É—à–∞—é...");
                   // –ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ AI –º–æ–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥–Ω—è—Ç—å –ø–æ—Ä–æ–≥ –≤—Ä–µ–º–µ–Ω–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å —ç—Ö–æ
                };
            }
        } else {
            setStatus("üéôÔ∏è –°–ª—É—à–∞—é...");
        }
    });

    return () => {
        socketRef.current?.disconnect();
        stopAudio();
    };
  }, []);

  const calibrateNoiseLevel = () => {
    return new Promise<void>((resolve) => {
      setCalibrating(true);
      setStatus("ü§´ –¢–ò–®–ò–ù–ê! –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —à—É–º–∞...");
      calibrationSamplesRef.current = [];
      
      const calibrate = () => {
        if (!analyserRef.current) return;
        
        const bufferLength = analyserRef.current.fftSize;
        const dataArray = new Uint8Array(bufferLength);
        analyserRef.current.getByteTimeDomainData(dataArray);
        
        let sum = 0;
        for (let i = 0; i < bufferLength; i++) {
            const val = (dataArray[i] - 128) / 128;
            sum += val * val;
        }
        const rms = Math.sqrt(sum / bufferLength);
        const currentVol = rms * 100;
        
        calibrationSamplesRef.current.push(currentVol);
        setVolume(currentVol); // Visual feedback
        
        if (calibrationSamplesRef.current.length >= 60) { // ~1 sec
          const avgNoise = calibrationSamplesRef.current.reduce((a, b) => a + b) / calibrationSamplesRef.current.length;
          
          // –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ñ–µ—Å—Ç–∫–∏–π –º–∏–Ω–∏–º—É–º 10. –ï—Å–ª–∏ —à—É–º 0.5, –ø–æ—Ä–æ–≥ –±—É–¥–µ—Ç 10. –ï—Å–ª–∏ —à—É–º 8, –ø–æ—Ä–æ–≥ 13.
          const calculatedThreshold = Math.max(avgNoise + 5, 10);
          
          noiseLevelRef.current = calculatedThreshold;
          setThresholdDisplay(calculatedThreshold);
          
          console.log(`[Calibration] Avg Noise: ${avgNoise.toFixed(2)}, Set Threshold: ${calculatedThreshold}`);
          setCalibrating(false);
          setStatus("üéôÔ∏è –°–ª—É—à–∞—é...");
          resolve();
        } else {
          requestAnimationFrame(calibrate);
        }
      };
      
      calibrate();
    });
  };

  const startCall = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;

      const deviceInfo = {
        userAgent: navigator.userAgent,
        timestamp: new Date().toISOString()
      };
      socketRef.current?.emit("call-ai", { deviceInfo });

    } catch (err) {
      console.error(err);
      setStatus("‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É");
    }
  };

  const startListening = async () => {
      if (!mediaStreamRef.current) return;

      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
      const ctx = new AudioContextClass({ sampleRate: 16000 });
      audioContextRef.current = ctx;

      try {
        await ctx.audioWorklet.addModule('/audio-processor.js'); 
      } catch (e) {
        console.error("Failed to load audio-processor.js", e);
        return;
      }
      
      const source = ctx.createMediaStreamSource(mediaStreamRef.current);
      const analyser = ctx.createAnalyser();
      analyser.fftSize = 1024; // 512 bins
      analyser.smoothingTimeConstant = 0.3;
      analyserRef.current = analyser;

      const processor = new AudioWorkletNode(ctx, 'audio-recorder-processor');
      processorRef.current = processor;

      processor.port.onmessage = (e) => {
          if (e.data.type === 'audioChunk' && isSpeakingRef.current) {
              audioChunksRef.current.push(e.data.chunk);
          }
      };

      source.connect(analyser);
      source.connect(processor);
      processor.connect(ctx.destination); 

      await calibrateNoiseLevel();
      detectVoiceActivity();
  };

  const detectVoiceActivity = () => {
      if (!analyserRef.current || !mediaStreamRef.current?.active) return;

      const bufferLength = analyserRef.current.fftSize;
      const dataArray = new Uint8Array(bufferLength);
      analyserRef.current.getByteTimeDomainData(dataArray);

      let sum = 0;
      for (let i = 0; i < bufferLength; i++) {
          const val = (dataArray[i] - 128) / 128; 
          sum += val * val;
      }
      const rms = Math.sqrt(sum / bufferLength);
      const currentVol = rms * 100; 
      setVolume(currentVol);

      const THRESHOLD = noiseLevelRef.current;

      if (currentVol > THRESHOLD) {
          if (!isSpeakingRef.current) {
              console.log("üó£Ô∏è Speech started (Vol: " + currentVol.toFixed(1) + ")");
              isSpeakingRef.current = true;
              setIsRecording(true);
              audioChunksRef.current = [];
              processorRef.current?.port.postMessage({ type: 'startRecording' });
              
              if (silenceTimerRef.current) clearTimeout(silenceTimerRef.current);

              // –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–µ–¥–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å. –ï—Å–ª–∏ –≥–æ–≤–æ—Ä–∏–º > 7 —Å–µ–∫, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ.
              if (maxDurationTimerRef.current) clearTimeout(maxDurationTimerRef.current);
              maxDurationTimerRef.current = setTimeout(() => {
                  console.log("‚ö†Ô∏è Max duration reached (7s), forcing send...");
                  stopRecordingAndSend();
              }, 7000);
          }
          // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä —Ç–∏—à–∏–Ω—ã, –ø–æ–∫–∞ –≥–æ–≤–æ—Ä–∏–º
          if (silenceTimerRef.current) clearTimeout(silenceTimerRef.current);
          
      } else {
          // –ï—Å–ª–∏ –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏, –∞ —Ç–µ–ø–µ—Ä—å —Ç–∏—à–∏–Ω–∞
          if (isSpeakingRef.current && !silenceTimerRef.current) {
              // –ñ–¥–µ–º 800–º—Å —Ç–∏—à–∏–Ω—ã
              silenceTimerRef.current = setTimeout(() => {
                  console.log("ü§´ Silence detected, sending...");
                  stopRecordingAndSend();
              }, 800); 
          }
      }

      requestAnimationFrame(detectVoiceActivity);
  };

  const stopRecordingAndSend = async () => {
      // –û—á–∏—Å—Ç–∫–∞ —Ç–∞–π–º–µ—Ä–æ–≤
      if (silenceTimerRef.current) { clearTimeout(silenceTimerRef.current); silenceTimerRef.current = null; }
      if (maxDurationTimerRef.current) { clearTimeout(maxDurationTimerRef.current); maxDurationTimerRef.current = null; }
      
      isSpeakingRef.current = false;
      setIsRecording(false);
      processorRef.current?.port.postMessage({ type: 'stopRecording' });

      if (audioChunksRef.current.length === 0) return;

      const flat = new Float32Array(audioChunksRef.current.reduce((acc, val) => acc + val.length, 0));
      let offset = 0;
      for (const chunk of audioChunksRef.current) {
          flat.set(chunk, offset);
          offset += chunk.length;
      }
      
      // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ "–≤—Å–ø–ª–µ—Å–∫–∏" (–º–µ–Ω—å—à–µ 0.3 —Å–µ–∫)
      if (flat.length < 16000 * 0.3) {
          console.log("Ignored short noise (<0.3s)");
          audioChunksRef.current = [];
          return;
      }

      const buffer = flat.buffer.slice(flat.byteOffset, flat.byteOffset + flat.byteLength);
      const base64 = arrayBufferToBase64(buffer);

      if (sessionIdRef.current && socketRef.current) {
        console.log(`Sending audio chunk (${base64.length} bytes)...`);
        socketRef.current.emit("audio-chunk", {
            sessionId: sessionIdRef.current, 
            audioData: base64,
            sampleRate: 16000,
            channels: 1,
            isFinal: true
        });
        setStatus("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞...");
      }
      
      audioChunksRef.current = [];
  };

  const stopAudio = () => {
      audioContextRef.current?.close();
      mediaStreamRef.current?.getTracks().forEach(t => t.stop());
      setIsCalling(false);
      isSpeakingRef.current = false;
      if (silenceTimerRef.current) clearTimeout(silenceTimerRef.current);
      if (maxDurationTimerRef.current) clearTimeout(maxDurationTimerRef.current);
  };

  const handleEndCall = () => {
      if (sessionIdRef.current && socketRef.current) {
        socketRef.current.emit("end-ai-call", { sessionId: sessionIdRef.current }); 
      }
      stopAudio();
      setStatus("–ó–≤–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω");
      sessionIdRef.current = null;
  };

  return (
    <Box sx={{ p: 3, maxWidth: 800, margin: '0 auto' }}>
      <Typography variant="h4" gutterBottom>üìû NG911 –ì–æ–ª–æ—Å–æ–≤–æ–π –¢–µ—Ä–º–∏–Ω–∞–ª</Typography>
      
      <Paper sx={{ p: 3, mb: 3, textAlign: 'center', background: isCalling ? '#e3f2fd' : '#fff' }}>
        <Typography variant="h6" sx={{ mb: 2, color: calibrating ? '#ff9800' : 'inherit' }}>
          {status}
        </Typography>
        
        {/* –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥—Ä–æ–º–∫–æ—Å—Ç–∏ */}
        <Box sx={{ width: '80%', margin: '0 auto 20px' }}>
            <Box sx={{ display: 'flex', justifyContent: 'space-between', mb: 0.5 }}>
                <Typography variant="caption">Mic Level: {volume.toFixed(1)}</Typography>
                <Typography variant="caption" color="error">Threshold: {thresholdDisplay.toFixed(1)}</Typography>
            </Box>
            <LinearProgress 
                variant="determinate" 
                value={Math.min(volume * 2, 100)} 
                color={isRecording ? "error" : volume > thresholdDisplay ? "warning" : "primary"}
                sx={{ height: 10, borderRadius: 5 }}
            />
        </Box>

        <Box sx={{ position: 'relative', display: 'inline-block', mb: 2 }}>
            <div style={{
                width: 100, height: 100, borderRadius: '50%',
                background: isRecording ? '#ef5350' : '#2196f3',
                display: 'flex', alignItems: 'center', justifyContent: 'center',
                transition: '0.1s',
                transform: `scale(${1 + Math.min(volume/50, 0.3)})`,
                boxShadow: `0 0 ${volume * 2}px ${isRecording ? 'red' : '#2196f333'}` 
            }}>
                {isRecording ? <RecordVoiceOver style={{ fontSize: 50, color: 'white' }} /> : <Mic style={{ fontSize: 50, color: 'white' }} />}
            </div>
        </Box>

        <Stack direction="row" spacing={2} justifyContent="center" mt={2}>
            {!isCalling ? (
                <Button variant="contained" size="large" color="primary" startIcon={<SettingsVoice />} onClick={startCall}>
                    –ù–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫
                </Button>
            ) : (
                <Button variant="contained" size="large" color="error" startIcon={<PhoneDisabled />} onClick={handleEndCall}>
                    –ó–∞–≤–µ—Ä—à–∏—Ç—å
                </Button>
            )}
        </Stack>
      </Paper>

      <Paper sx={{ p: 2, height: 300, overflowY: 'auto', mb: 3, bgcolor: '#f5f5f5' }}>
          {transcript.length === 0 && <Typography color="text.secondary" align="center">–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞...</Typography>}
          {transcript.map((msg, i) => (
              <Box key={i} sx={{ 
                  display: 'flex', 
                  justifyContent: msg.role === 'user' ? 'flex-end' : 'flex-start',
                  mb: 1 
              }}>
                  <Paper sx={{ 
                      p: 1.5, 
                      bgcolor: msg.role === 'user' ? '#1976d2' : '#fff',
                      color: msg.role === 'user' ? '#fff' : '#000',
                      maxWidth: '80%'
                  }}>
                      <Typography variant="body1">{msg.text}</Typography>
                  </Paper>
              </Box>
          ))}
      </Paper>

      {incidentData && (
          <Card variant="outlined">
              <CardContent>
                  <Typography variant="h6">üìã –î–∞–Ω–Ω—ã–µ (Live)</Typography>
                  <Stack direction="row" spacing={1} my={1}>
                      <Chip label={incidentData.priority?.toUpperCase()} color={incidentData.priority === 'critical' ? 'error' : 'warning'} />
                      <Chip label={incidentData.category} />
                  </Stack>
                  <Typography><b>–ê–¥—Ä–µ—Å:</b> {incidentData.address}</Typography>
              </CardContent>
          </Card>
      )}

      <audio ref={aiAudioRef} style={{ display: 'none' }} />
    </Box>
  );
};

export default CallSimulator;